"""
main.py - Financial Fraud Detection Model Training & Evaluation
==============================================================

Professional model training pipeline using real financial datasets.
Loads data from data_loader.py and trains RTX 4070 optimized models.

Features:
- Real financial data only (no synthetic data)
- Professional train/validation/test splits
- GPU optimization for RTX 4070
- Comprehensive evaluation metrics
- Berlin fintech deployment ready

Author: Efe Demir Civelek
Usage: python main.py
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, 
                           roc_auc_score, precision_recall_fscore_support, roc_curve)
import os
import time
import re
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# Import our data loader
try:
    from data_loader import RealFinancialDataLoader
except ImportError:
    print("âŒ Please ensure data_loader.py is in the same directory")
    exit(1)

# =============================================================================
# CONFIGURATION
# =============================================================================

CONFIG = {
    'model': {
        'embedding_dim': 128,
        'lstm_hidden': 64,
        'num_layers': 2,
        'dropout': 0.4,
        'max_sequence_length': 150
    },
    'training': {
        'batch_size': 64,  # Optimized for RTX 4070
        'learning_rate': 0.001,
        'weight_decay': 1e-4,
        'max_epochs': 15,
        'patience': 4
    },
    'data': {
        'test_size': 0.2,
        'val_size': 0.15,
        'min_word_freq': 2,
        'random_state': 42
    }
}

# =============================================================================
# GPU OPTIMIZATION
# =============================================================================

def setup_rtx4070_optimization() -> torch.device:
    """Setup RTX 4070 specific optimizations."""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        # RTX 4070 optimizations
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        
        device = torch.device('cuda:0')
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        print(f"ðŸ”¥ GPU Detected: {gpu_name}")
        print(f"ðŸ’¾ GPU Memory: {gpu_memory:.1f}GB")
        print("âš¡ RTX 4070 optimizations enabled")
        
        return device
    else:
        print("âš ï¸ GPU not available, using CPU")
        return torch.device('cpu')

# =============================================================================
# FEATURE EXTRACTION
# =============================================================================

class FinancialFeatureExtractor:
    """Extract financial domain-specific features from real data."""
    
    def __init__(self):
        # Based on real financial fraud research
        self.fraud_indicators = {
            'uncertainty': ['may', 'might', 'could', 'possible', 'potential', 'allegedly'],
            'secrecy': ['confidential', 'insider', 'exclusive', 'secret', 'undisclosed'],
            'urgency': ['urgent', 'immediate', 'quickly', 'soon', 'deadline', 'limited'],
            'exaggeration': ['unprecedented', 'revolutionary', 'breakthrough', 'massive'],
            'vague_sources': ['sources say', 'reports indicate', 'rumors', 'unnamed'],
        }
        
        self.legitimate_indicators = {
            'official': ['announced', 'reported', 'filed', 'disclosed', 'confirmed'],
            'specific': ['percent', 'million', 'billion', 'quarter', 'fiscal year'],
            'regulatory': ['SEC', 'filing', 'compliance', 'regulatory', 'audit'],
            'verified': ['earnings', 'revenue', 'profit', 'consensus', 'analyst']
        }
    
    def extract_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Extract comprehensive features from real financial text."""
        print("ðŸ” Extracting financial domain features...")
        
        df = df.copy()
        
        # Fraud indicator scores
        for category, keywords in self.fraud_indicators.items():
            df[f'{category}_score'] = df['text'].apply(
                lambda x: self._calculate_keyword_score(x, keywords)
            )
        
        # Legitimate indicator scores
        for category, keywords in self.legitimate_indicators.items():
            df[f'{category}_score'] = df['text'].apply(
                lambda x: self._calculate_keyword_score(x, keywords)
            )
        
        # Text characteristics
        df['text_length'] = df['text'].str.len()
        df['word_count'] = df['text'].str.split().str.len()
        df['sentence_count'] = df['text'].str.count(r'[.!?]+') + 1
        df['avg_word_length'] = df['text'].apply(self._avg_word_length)
        
        # Financial-specific features
        df['number_count'] = df['text'].str.count(r'\d+')
        df['percentage_count'] = df['text'].str.count(r'\d+%')
        df['dollar_count'] = df['text'].str.count(r'\$[\d,]+')
        df['capital_ratio'] = df['text'].apply(self._capital_ratio)
        
        # Source-based features
        df['source_credibility'] = df['source'].apply(self._source_credibility)
        df['is_sec_filing'] = df['source'].str.contains('SEC', case=False, na=False).astype(int)
        df['is_news_source'] = df['source'].str.contains('Bank', case=False, na=False).astype(int) | df['source'].str.contains('News', case=False, na=False).astype(int)
        
        print(f"âœ… Extracted features from {len(df)} real financial articles")
        
        return df
    
    def _calculate_keyword_score(self, text: str, keywords: List[str]) -> float:
        """Calculate normalized keyword frequency."""
        if pd.isna(text):
            return 0.0
        
        text_lower = str(text).lower()
        count = sum(1 for keyword in keywords if keyword in text_lower)
        words = len(text_lower.split())
        
        return count / (words + 1) * 100
    
    def _avg_word_length(self, text: str) -> float:
        """Calculate average word length."""
        if pd.isna(text):
            return 0.0
        
        words = str(text).split()
        return sum(len(word) for word in words) / len(words) if words else 0.0
    
    def _capital_ratio(self, text: str) -> float:
        """Calculate ratio of capital letters."""
        if pd.isna(text) or len(str(text)) == 0:
            return 0.0
        
        text_str = str(text)
        capitals = sum(1 for c in text_str if c.isupper())
        return capitals / len(text_str)
    
    def _source_credibility(self, source: str) -> float:
        """Assign credibility score based on source type."""
        if pd.isna(source):
            return 0.5
        
        source_lower = str(source).lower()
        
        if 'sec' in source_lower:
            return 0.95  # SEC filings are official
        elif any(term in source_lower for term in ['phrase bank', 'reuters', 'bloomberg']):
            return 0.90  # High-quality journalism
        elif 'financial' in source_lower:
            return 0.80  # Financial sources
        else:
            return 0.60  # Other sources

# =============================================================================
# SECTION NAME
# =============================================================================

class FinancialFraudLSTM(nn.Module):
    """
    Professional LSTM model for financial fraud detection.
    
    Architecture optimized for real financial text characteristics:
    - Bidirectional LSTM for context understanding
    - Financial feature integration
    - Dropout regularization to prevent overfitting
    - Batch normalization for stable training
    """
    
    def __init__(self, vocab_size: int, embedding_dim: int = 128, 
                 lstm_hidden: int = 64, num_layers: int = 2,
                 feature_dim: int = 20, dropout: float = 0.4):
        super(FinancialFraudLSTM, self).__init__()
        
        # Text processing
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.embedding_dropout = nn.Dropout(dropout)
        
        self.lstm = nn.LSTM(
            embedding_dim, lstm_hidden, num_layers,
            batch_first=True, dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )
        
        # Feature processing
        self.feature_processor = nn.Sequential(
            nn.Linear(feature_dim, 32),
            nn.ReLU(),
            nn.BatchNorm1d(32),
            nn.Dropout(dropout),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Classification
        fusion_dim = lstm_hidden * 2 + 16  # bidirectional + features
        self.classifier = nn.Sequential(
            nn.Linear(fusion_dim, 64),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Dropout(dropout),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.BatchNorm1d(32),
            nn.Dropout(dropout),
            nn.Linear(32, 1)
        )
        
    def forward(self, text_input: torch.Tensor, features: torch.Tensor) -> torch.Tensor:
        """Forward pass through the model."""
        # Text processing
        embedded = self.embedding(text_input)
        embedded = self.embedding_dropout(embedded)
        
        lstm_out, (hidden, _) = self.lstm(embedded)
        
        # Use mean pooling over sequence length for robustness
        text_repr = torch.mean(lstm_out, dim=1)
        
        # Feature processing
        feature_repr = self.feature_processor(features)
        
        # Fusion and classification
        combined = torch.cat([text_repr, feature_repr], dim=1)
        output = torch.sigmoid(self.classifier(combined))
        
        return output
    
# =============================================================================
# DATASET CLASS
# =============================================================================

class FinancialFraudDataset(Dataset):
    """PyTorch Dataset for real financial fraud detection."""
    
    def __init__(self, texts: List[str], features: np.ndarray, labels: List[int],
                 vocab_to_idx: Dict[str, int], max_length: int = 150):
        self.texts = texts
        self.features = features
        self.labels = labels
        self.vocab_to_idx = vocab_to_idx
        self.max_length = max_length
        
    def __len__(self) -> int:
        return len(self.texts)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        # Clean and tokenize text
        text = str(self.texts[idx]).lower()
        words = re.findall(r'\w+', text)[:self.max_length]
        indices = [self.vocab_to_idx.get(word, self.vocab_to_idx['<UNK>']) for word in words]
        
        # Pad sequence
        if len(indices) < self.max_length:
            indices.extend([0] * (self.max_length - len(indices)))
        
        return (
            torch.tensor(indices, dtype=torch.long),
            torch.tensor(self.features[idx], dtype=torch.float32),
            torch.tensor(self.labels[idx], dtype=torch.float32)
        )

# =============================================================================
# TRAINING ENGINE
# =============================================================================

class FraudDetectionTrainer:
    """Professional training engine for financial fraud detection."""
    
    def __init__(self, device: torch.device):
        self.device = device
        
    def create_vocabulary(self, texts: List[str], min_freq: int = 2) -> Dict[str, int]:
        """Create vocabulary from real financial texts."""
        print("ðŸ“š Building vocabulary from real financial data...")
        
        word_freq = {}
        for text in texts:
            words = re.findall(r'\w+', str(text).lower())
            for word in words:
                if len(word) > 1:  # Filter out single characters
                    word_freq[word] = word_freq.get(word, 0) + 1
        
        # Build vocabulary with frequency filtering
        vocab_to_idx = {'<PAD>': 0, '<UNK>': 1}
        idx = 2
        
        for word, freq in word_freq.items():
            if freq >= min_freq:
                vocab_to_idx[word] = idx
                idx += 1
        
        print(f"âœ… Vocabulary created: {len(vocab_to_idx):,} words")
        return vocab_to_idx
    
    def prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:
        """Prepare and normalize features."""
        feature_cols = [col for col in df.columns 
                       if col.endswith('_score') or col in [
                           'text_length', 'word_count', 'sentence_count', 'avg_word_length',
                           'number_count', 'percentage_count', 'dollar_count', 'capital_ratio',
                           'source_credibility', 'is_sec_filing', 'is_news_source'
                       ]]
        
        print(f"ðŸ“Š Using {len(feature_cols)} features: {feature_cols}")
        
        features = df[feature_cols].fillna(0).values
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        return features_scaled, feature_cols
    
    def train_model(self, df: pd.DataFrame) -> Dict:
        """Train fraud detection model on real financial data."""
        print("\nðŸš€ TRAINING FINANCIAL FRAUD DETECTION MODEL")
        print("="*60)
        
        # Extract features
        feature_extractor = FinancialFeatureExtractor()
        df_enhanced = feature_extractor.extract_features(df)
        
        # Prepare data
        texts = df_enhanced['text'].astype(str).tolist()
        labels = df_enhanced['is_fraudulent'].astype(int).tolist()
        features, feature_names = self.prepare_features(df_enhanced)
        
        print(f"ðŸ“Š Dataset: {len(texts):,} articles")
        print(f"ðŸ“Š Features: {features.shape[1]}")
        print(f"ðŸ“Š Class distribution: {np.bincount(labels)}")
        
        # Create vocabulary
        vocab_to_idx = self.create_vocabulary(texts, CONFIG['data']['min_word_freq'])
        
        # Stratified splits for balanced training
        X_train, X_temp, y_train, y_temp, feat_train, feat_temp = train_test_split(
            texts, labels, features,
            test_size=CONFIG['data']['test_size'] + CONFIG['data']['val_size'],
            random_state=CONFIG['data']['random_state'],
            stratify=labels
        )
        
        X_val, X_test, y_val, y_test, feat_val, feat_test = train_test_split(
            X_temp, y_temp, feat_temp,
            test_size=CONFIG['data']['test_size'] / (CONFIG['data']['test_size'] + CONFIG['data']['val_size']),
            random_state=CONFIG['data']['random_state'],
            stratify=y_temp
        )
        
        print(f"ðŸ“Š Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
        
        # Create datasets
        train_dataset = FinancialFraudDataset(X_train, feat_train, y_train, vocab_to_idx)
        val_dataset = FinancialFraudDataset(X_val, feat_val, y_val, vocab_to_idx)
        test_dataset = FinancialFraudDataset(X_test, feat_test, y_test, vocab_to_idx)
        
        # RTX 4070 optimized data loaders
        train_loader = DataLoader(
            train_dataset, 
            batch_size=CONFIG['training']['batch_size'], 
            shuffle=True,
            num_workers=0, 
            pin_memory=True
        )
        val_loader = DataLoader(
            val_dataset, 
            batch_size=CONFIG['training']['batch_size'], 
            shuffle=False,
            num_workers=0, 
            pin_memory=True
        )
        test_loader = DataLoader(
            test_dataset, 
            batch_size=CONFIG['training']['batch_size'], 
            shuffle=False
        )
        
        # Initialize model
        model = FinancialFraudLSTM(
            vocab_size=len(vocab_to_idx),
            embedding_dim=CONFIG['model']['embedding_dim'],
            lstm_hidden=CONFIG['model']['lstm_hidden'],
            num_layers=CONFIG['model']['num_layers'],
            feature_dim=features.shape[1],
            dropout=CONFIG['model']['dropout']
        ).to(self.device)
        
        # Training setup
        criterion = nn.BCELoss()
        optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=CONFIG['training']['learning_rate'],
            weight_decay=CONFIG['training']['weight_decay']
        )
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)
        
        print(f"ðŸ¤– Model parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # Training loop
        results = self._train_loop(model, train_loader, val_loader, criterion, optimizer, scheduler)
        
        # Final evaluation
        test_results = self._evaluate_model(model, test_loader)
        results.update(test_results)
        
        # Add artifacts
        results.update({
            'model': model,
            'vocab': vocab_to_idx,
            'feature_names': feature_names,
            'config': CONFIG
        })
        
        return results
    
    def _train_loop(self, model, train_loader, val_loader, criterion, optimizer, scheduler) -> Dict:
        """Main training loop with early stopping."""
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        train_losses, val_losses = [], []
        train_accs, val_accs = [], []
        
        print(f"\nðŸŽ¯ Training for up to {CONFIG['training']['max_epochs']} epochs...")
        
        for epoch in range(CONFIG['training']['max_epochs']):
            # Training phase
            model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            for batch_idx, (text_batch, feat_batch, label_batch) in enumerate(train_loader):
                text_batch = text_batch.to(self.device, non_blocking=True)
                feat_batch = feat_batch.to(self.device, non_blocking=True)
                label_batch = label_batch.to(self.device, non_blocking=True)
                
                optimizer.zero_grad()
                outputs = model(text_batch, feat_batch).squeeze()
                loss = criterion(outputs, label_batch)
                loss.backward()
                
                # Gradient clipping for stability
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += loss.item()
                predicted = (outputs > 0.5).float()
                train_total += label_batch.size(0)
                train_correct += (predicted == label_batch).sum().item()
            
            # Validation phase
            model.eval()
            val_loss = 0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for text_batch, feat_batch, label_batch in val_loader:
                    text_batch = text_batch.to(self.device, non_blocking=True)
                    feat_batch = feat_batch.to(self.device, non_blocking=True)
                    label_batch = label_batch.to(self.device, non_blocking=True)
                    
                    outputs = model(text_batch, feat_batch).squeeze()
                    loss = criterion(outputs, label_batch)
                    
                    val_loss += loss.item()
                    predicted = (outputs > 0.5).float()
                    val_total += label_batch.size(0)
                    val_correct += (predicted == label_batch).sum().item()
            
            # Calculate metrics
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            train_acc = train_correct / train_total
            val_acc = val_correct / val_total
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            train_accs.append(train_acc)
            val_accs.append(val_acc)
            
            scheduler.step(avg_val_loss)
            
            print(f"Epoch [{epoch+1:2d}] Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, "
                  f"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}")
            
            # Early stopping
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
                torch.save(model.state_dict(), 'best_financial_fraud_model.pth')
            else:
                patience_counter += 1
                
            if patience_counter >= CONFIG['training']['patience']:
                print(f"Early stopping at epoch {epoch+1}")
                break
        
        # Load best model
        model.load_state_dict(torch.load('best_financial_fraud_model.pth'))
        
        return {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_accs': train_accs,
            'val_accs': val_accs
        }
    
    def _evaluate_model(self, model, test_loader) -> Dict:
        """Comprehensive model evaluation."""
        model.eval()
        
        all_predictions = []
        all_probabilities = []
        all_labels = []
        
        with torch.no_grad():
            for text_batch, feat_batch, label_batch in test_loader:
                text_batch = text_batch.to(self.device, non_blocking=True)
                feat_batch = feat_batch.to(self.device, non_blocking=True)
                
                outputs = model(text_batch, feat_batch).squeeze()
                predictions = (outputs > 0.5).float()
                
                all_predictions.extend(predictions.cpu().numpy())
                all_probabilities.extend(outputs.cpu().numpy())
                all_labels.extend(label_batch.numpy())
        
        # Calculate comprehensive metrics
        accuracy = accuracy_score(all_labels, all_predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')
        
        try:
            roc_auc = roc_auc_score(all_labels, all_probabilities)
        except:
            roc_auc = 0.5
        
        return {
            'test_accuracy': accuracy,
            'test_precision': precision,
            'test_recall': recall,
            'test_f1': f1,
            'test_roc_auc': roc_auc,
            'test_predictions': all_predictions,
            'test_probabilities': all_probabilities,
            'test_labels': all_labels
        }

# =============================================================================
# VISUALIZATION & REPORTING
# =============================================================================

class ResultsVisualizer:
    """Create professional visualizations and reports."""
    
    @staticmethod
    def plot_training_results(results: Dict) -> None:
        """Create comprehensive training visualizations."""
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Training curves
        epochs = range(1, len(results['train_losses']) + 1)
        
        axes[0, 0].plot(epochs, results['train_losses'], 'b-', label='Training', linewidth=2)
        axes[0, 0].plot(epochs, results['val_losses'], 'r-', label='Validation', linewidth=2)
        axes[0, 0].set_title('Loss Curves', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        axes[0, 1].plot(epochs, results['train_accs'], 'b-', label='Training', linewidth=2)
        axes[0, 1].plot(epochs, results['val_accs'], 'r-', label='Validation', linewidth=2)
        axes[0, 1].set_title('Accuracy Curves', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Confusion Matrix
        cm = confusion_matrix(results['test_labels'], results['test_predictions'])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],
                   xticklabels=['Legitimate', 'Fraudulent'],
                   yticklabels=['Legitimate', 'Fraudulent'])
        axes[1, 0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')
        axes[1, 0].set_ylabel('True Label')
        axes[1, 0].set_xlabel('Predicted Label')
        
        # ROC Curve
        try:
            fpr, tpr, _ = roc_curve(results['test_labels'], results['test_probabilities'])
            axes[1, 1].plot(fpr, tpr, 'b-', linewidth=2, 
                           label=f'ROC (AUC = {results["test_roc_auc"]:.3f})')
            axes[1, 1].plot([0, 1], [0, 1], 'r--', alpha=0.5)
            axes[1, 1].set_title('ROC Curve', fontsize=14, fontweight='bold')
            axes[1, 1].set_xlabel('False Positive Rate')
            axes[1, 1].set_ylabel('True Positive Rate')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        except:
            axes[1, 1].text(0.5, 0.5, 'ROC curve unavailable', 
                           ha='center', va='center', fontsize=12)
        
        plt.tight_layout()
        plt.show()
    
    @staticmethod
    def print_performance_report(results: Dict) -> None:
        """Print comprehensive performance report."""
        print("\n" + "="*70)
        print("FINANCIAL FRAUD DETECTION - PERFORMANCE REPORT")
        print("="*70)
        
        print(f"ðŸ“Š Test Results on Real Financial Data:")
        print(f"   Accuracy:  {results['test_accuracy']:.4f} ({results['test_accuracy']*100:.1f}%)")
        print(f"   Precision: {results['test_precision']:.4f}")
        print(f"   Recall:    {results['test_recall']:.4f}")
        print(f"   F1-Score:  {results['test_f1']:.4f}")
        print(f"   ROC-AUC:   {results['test_roc_auc']:.4f}")
        
        # Performance interpretation
        accuracy = results['test_accuracy']
        if accuracy >= 0.90:
            grade = "ðŸ† Excellent"
        elif accuracy >= 0.85:
            grade = "ðŸ¥ˆ Very Good"
        elif accuracy >= 0.80:
            grade = "ðŸ¥‰ Good"
        else:
            grade = "ðŸ“ˆ Acceptable"
        
        print(f"\nðŸŽ¯ Performance Grade: {grade}")
        
        # Berlin fintech applications
        print(f"\nðŸ¦ Berlin Fintech Deployment Readiness:")
        companies = ['N26', 'Trade Republic', 'Klarna', 'SumUp', 'Raisin']
        for company in companies:
            status = "âœ… Ready" if accuracy >= 0.85 else "âš ï¸ Needs improvement"
            print(f"   {company}: {status}")
        
        print("="*70)

def demonstrate_predictions(results: Dict, df: pd.DataFrame) -> None:
    """Demonstrate model predictions on real examples."""
    print("\nðŸŽ¯ FRAUD DETECTION DEMONSTRATION")
    print("="*50)
    
    # Get some real examples for demonstration
    fraud_examples = df[df['is_fraudulent'] == 1].sample(n=3, random_state=42)
    legit_examples = df[df['is_fraudulent'] == 0].sample(n=3, random_state=42)
    
    all_examples = pd.concat([legit_examples, fraud_examples])
    
    for i, (_, row) in enumerate(all_examples.iterrows()):
        true_label = "Fraudulent" if row['is_fraudulent'] else "Legitimate"
        text = row['text'][:200] + "..." if len(row['text']) > 200 else row['text']
        source = row['source']
        
        print(f"\nExample {i+1}: {true_label}")
        print(f"Source: {source}")
        print(f"Text: \"{text}\"")
        print("-" * 50)

# =============================================================================
# MAIN EXECUTION
# =============================================================================

def main():
    """Main training pipeline."""
    print("ðŸš€ Financial Fraud Detection Training Pipeline")
    print("ðŸ“Š Using Real Financial Data Only")
    print("="*60)
    
    # Setup GPU
    device = setup_rtx4070_optimization()
    
    # Load real financial data
    print("\nðŸ“‚ Phase 1: Loading Real Financial Data")
    
    # Try to load existing dataset
    if os.path.exists('real_financial_fraud_dataset.csv'):
        print("âœ… Found existing dataset")
        df = pd.read_csv('real_financial_fraud_dataset.csv')
    else:
        print("ðŸ“¥ Creating new dataset...")
        loader = RealFinancialDataLoader()
        df = loader.load_all_real_datasets()
        loader.save_dataset(df)
    
    print(f"ðŸ“Š Dataset loaded: {len(df):,} real financial articles")
    
    # Train model
    print("\nðŸ¤– Phase 2: Model Training")
    trainer = FraudDetectionTrainer(device)
    results = trainer.train_model(df)
    
    # Visualize results
    print("\nðŸ“ˆ Phase 3: Results Analysis")
    ResultsVisualizer.plot_training_results(results)
    ResultsVisualizer.print_performance_report(results)
    
    # Demonstrate predictions
    print("\nðŸŽ¯ Phase 4: Prediction Demonstration")
    demonstrate_predictions(results, df)
    
    # Save final model
    print("\nðŸ’¾ Phase 5: Saving Model")
    torch.save({
        'model_state_dict': results['model'].state_dict(),
        'vocab': results['vocab'],
        'feature_names': results['feature_names'],
        'config': CONFIG,
        'results': {k: v for k, v in results.items() if k not in ['model']}
    }, 'financial_fraud_detector.pth')
    
    print("âœ… Model saved as 'financial_fraud_detector.pth'")
    print("ðŸŽ‰ Training completed successfully!")
    print("\nðŸ¦ Ready for Berlin Fintech Deployment!")
    
    return results

if __name__ == "__main__":
    # Run complete training pipeline
    try:
        results = main()
        
        final_accuracy = results['test_accuracy']
        print(f"\nðŸ† FINAL RESULTS:")
        print(f"   Test Accuracy: {final_accuracy:.1%}")
        print(f"   Model: Professional-grade fraud detection")
        print(f"   Data: Real financial industry sources only")
        print(f"   Status: Ready for production deployment")
        
    except Exception as e:
        print(f"\nâŒ Training failed: {e}")
        print("Please ensure:")
        print("1. data_loader.py is in the same directory")
        print("2. Required packages are installed")
        print("3. Internet connection for dataset download")
        print("\nInstall requirements:")
        print("pip install torch pandas numpy scikit-learn matplotlib seaborn datasets")

        
        def extract_features(self, df: pd.DataFrame) -> pd.DataFrame:
            """Extract comprehensive features from real financial text."""
            print("ðŸ” Extracting financial domain features...")
            
            df = df.copy()
            
            # Fraud indicator scores
            for category, keywords in self.fraud_indicators.items():
                df[f'{category}_score'] = df['text'].apply(
                    lambda x: self._calculate_keyword_score(x, keywords)
                )
            
            # Legitimate indicator scores
            for category, keywords in self.legitimate_indicators.items():
                df[f'{category}_score'] = df['text'].apply(
                    lambda x: self._calculate_keyword_score(x, keywords)
                )
            
            # Text characteristics
            df['text_length'] = df['text'].str.len()
            df['word_count'] = df['text'].str.split().str.len()
            df['sentence_count'] = df['text'].str.count(r'[.!?]+') + 1
            df['avg_word_length'] = df['text'].apply(self._avg_word_length)
            
            # Financial-specific features
            df['number_count'] = df['text'].str.count(r'\d+')
            df['percentage_count'] = df['text'].str.count(r'\d+%')
            df['dollar_count'] = df['text'].str.count(r'\$[\d,]+')
            df['capital_ratio'] = df['text'].apply(self._capital_ratio)
            
            # Source-based features
            df['source_credibility'] = df['source'].apply(self._source_credibility)
            df['is_sec_filing'] = df['source'].str.contains('SEC', case=False, na=False).astype(int)
            df['is_news_source'] = df['source'].str.contains('Bank', case=False, na=False).astype(int) | df['source'].str.contains('News', case=False, na=False).astype(int)
            
            print(f"âœ… Extracted features from {len(df)} real financial articles")
            
            return df
    
    def _calculate_keyword_score(self, text: str, keywords: List[str]) -> float:
        """Calculate normalized keyword frequency."""
        if pd.isna(text):
            return 0.0
        
        text_lower = str(text).lower()
        count = sum(1 for keyword in keywords if keyword in text_lower)
        words = len(text_lower.split())
        
        return count / (words + 1) * 100
    
    def _avg_word_length(self, text: str) -> float:
        """Calculate average word length."""
        if pd.isna(text):
            return 0.0
        
        words = str(text).split()
        return sum(len(word) for word in words) / len(words) if words else 0.0
    
    def _capital_ratio(self, text: str) -> float:
        """Calculate ratio of capital letters."""
        if pd.isna(text) or len(str(text)) == 0:
            return 0.0
        
        text_str = str(text)
        capitals = sum(1 for c in text_str if c.isupper())
        return capitals / len(text_str)
    
    def _source_credibility(self, source: str) -> float:
        """Assign credibility score based on source type."""
        if pd.isna(source):
            return 0.5
        
        source_lower = str(source).lower()
        
        if 'sec' in source_lower:
            return 0.95  # SEC filings are official
        elif any(term in source_lower for term in ['phrase bank', 'reuters', 'bloomberg']):
            return 0.90  # High-quality journalism
        elif 'financial' in source_lower:
            return 0.80  # Financial sources
        else:
            return 0.60  # Other sources

# =============================================================================
# MODEL ARCHITECTURE
# =============================================================================

class FinancialFraudLSTM(nn.Module):
    """
    Professional LSTM model for financial fraud detection.
    
    Architecture optimized for real financial text characteristics:
    - Bidirectional LSTM for context understanding
    - Financial feature integration
    - Dropout regularization to prevent overfitting
    - Batch normalization for stable training
    """
    
    def __init__(self, vocab_size: int, embedding_dim: int = 128, 
                 lstm_hidden: int = 64, num_layers: int = 2,
                 feature_dim: int = 20, dropout: float = 0.4):
        super(FinancialFraudLSTM, self).__init__()
        
        # Text processing
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.embedding_dropout = nn.Dropout(dropout)
        
        self.lstm = nn.LSTM(
            embedding_dim, lstm_hidden, num_layers,
            batch_first=True, dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )
        
        # Feature processing
        self.feature_processor = nn.Sequential(
            nn.Linear(feature_dim, 32),
            nn.ReLU(),
            nn.BatchNorm1d(32),
            nn.Dropout(dropout),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Classification
        fusion_dim = lstm_hidden * 2 + 16  # bidirectional + features
        self.classifier = nn.Sequential(
            nn.Linear(fusion_dim, 64),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Dropout(dropout),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.BatchNorm1d(32),
            nn.Dropout(dropout),
            nn.Linear(32, 1)
            )